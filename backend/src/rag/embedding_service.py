# Embedding service module 
import numpy as np
from sentence_transformers import SentenceTransformer
import logging
from typing import List, Optional
import torch

logger = logging.getLogger(__name__)

class EmbeddingService:
    """Servi√ßo de embeddings usando Sentence Transformers"""
    
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.model_name = model_name
        self.model = None
        self.embedding_dim = 384  # Dimens√£o do all-MiniLM-L6-v2
        self._load_model()
    
    def _load_model(self):
        """Carrega o modelo de embeddings"""
        try:
            logger.info(f"ü§ñ Carregando modelo: {self.model_name}")
            
            # Verificar se CUDA est√° dispon√≠vel
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"üîß Usando device: {device}")
            
            self.model = SentenceTransformer(self.model_name, device=device)
            
            logger.info(f"‚úÖ Modelo carregado com sucesso")
            
        except Exception as e:
            logger.error(f"‚ùå Falha ao carregar modelo de embeddings: {e}")
            logger.error("üí° Verifique a conex√£o com a internet, o nome do modelo ou se as depend√™ncias (torch, transformers) est√£o instaladas corretamente.")
            # N√£o lan√ßar a exce√ß√£o para permitir que a aplica√ß√£o inicie
            # A falha ser√° detectada quando o m√©todo generate_embeddings for chamado
            self.model = None
    
    def generate_embeddings(self, texts: List[str], batch_size: int = 32) -> Optional[List[List[float]]]:
        """Gera embeddings para uma lista de textos"""
        
        if self.model is None:
            logger.error("‚ùå Modelo de embedding n√£o carregado. N√£o √© poss√≠vel gerar embeddings.")
            return None
            
        try:
            logger.info(f"üîÑ Gerando embeddings para {len(texts)} textos")
            
            # Processar em batches para efici√™ncia
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                
                # Gerar embeddings
                batch_embeddings = self.model.encode(
                    batch_texts,
                    convert_to_tensor=False,
                    show_progress_bar=False,
                    normalize_embeddings=True  # L2 normalization para cosine similarity
                )
                
                # Converter para lista
                if isinstance(batch_embeddings, np.ndarray):
                    batch_embeddings = batch_embeddings.tolist()
                
                all_embeddings.extend(batch_embeddings)
                
                logger.debug(f"üìä Batch {i//batch_size + 1}: {len(batch_embeddings)} embeddings")
            
            logger.info(f"‚úÖ {len(all_embeddings)} embeddings gerados")
            return all_embeddings
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar embeddings: {e}")
            raise
    
    def generate_single_embedding(self, text: str) -> List[float]:
        """Gera embedding para um √∫nico texto"""
        embeddings = self.generate_embeddings([text])
        return embeddings[0] if embeddings else []
    
    def similarity(self, embedding1: List[float], embedding2: List[float]) -> float:
        """Calcula similaridade cosseno entre dois embeddings"""
        try:
            vec1 = np.array(embedding1)
            vec2 = np.array(embedding2)
            
            # Cosine similarity
            dot_product = np.dot(vec1, vec2)
            norm1 = np.linalg.norm(vec1)
            norm2 = np.linalg.norm(vec2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            similarity = dot_product / (norm1 * norm2)
            return float(similarity)
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao calcular similaridade: {e}")
            return 0.0 