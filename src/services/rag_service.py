import logging
from typing import Dict, Any, Optional, List
import time
from datetime import datetime

# Importar componentes RAG
from rag.document_processor import DocumentProcessor
from rag.embedding_service import EmbeddingService
from rag.vector_store import VectorStore
from rag.cache_manager import CacheManager
from rag.retrieval_engine import RetrievalEngine

logger = logging.getLogger(__name__)

class RAGService:
    """Servi√ßo principal de RAG para licita√ß√µes"""
    
    def __init__(self, db_manager, unified_processor, openai_api_key: str, 
                 redis_host: str = "localhost"):
        self.db_manager = db_manager
        self.unified_processor = unified_processor
        
        # Inicializar componentes RAG
        self.document_processor = DocumentProcessor()
        self.embedding_service = EmbeddingService()
        self.vector_store = VectorStore(db_manager)
        self.cache_manager = CacheManager(redis_host=redis_host)
        self.retrieval_engine = RetrievalEngine(openai_api_key)
        
        logger.info("‚úÖ RAGService inicializado")
    
    def process_or_query(self, licitacao_id: str, query: str) -> Dict[str, Any]:
        """Fun√ß√£o principal: processa documentos se necess√°rio e responde query"""
        try:
            logger.info(f"üöÄ Iniciando RAG para licita√ß√£o: {licitacao_id}")
            
            # 1. Verificar cache primeiro
            cached_result = self.cache_manager.get_cached_query_result(query, licitacao_id)
            if cached_result:
                logger.info("‚ö° Resultado encontrado no cache")
                return {
                    'success': True,
                    'cached': True,
                    **cached_result
                }
            
            # 2. Verificar se documentos est√£o vetorizados
            status = self.vector_store.check_vectorization_status(licitacao_id)
                         
            if not status.get('vetorizado_completo', False):
                # 3. Processar documentos se necess√°rio (inclui extra√ß√£o recursiva de ZIPs)
                logger.info("üìù Documentos n√£o vetorizados, iniciando processamento completo...")
                print("üìù Documentos n√£o vetorizados, iniciando processamento completo...")
                vectorization_result = self._vectorize_licitacao(licitacao_id)
                print(f"vectorization_result: {vectorization_result}")
                
                if not vectorization_result['success']:
                    # Incluir detalhes de diagn√≥stico no resultado
                    error_response = {
                        'success': False,
                        'error': vectorization_result.get('error'),
                        'processing_details': {
                            'action': vectorization_result.get('action', 'unknown'),
                            'suggestion': vectorization_result.get('suggestion'),
                            'licitacao_info': vectorization_result.get('licitacao_info')
                        }
                    }
                    return error_response
            
            # 4. Responder query
            response_result = self._answer_query(query, licitacao_id)
            
            # 5. Cachear resultado
            if response_result['success']:
                self.cache_manager.cache_query_result(
                    query, licitacao_id, response_result, ttl=3600
                )
                
                # Adicionar informa√ß√µes de processamento se documentos foram processados nesta sess√£o
                if not status.get('vetorizado_completo', False):
                    response_result['processing_info'] = {
                        'documents_processed_this_session': True,
                        'processing_method': 'recursive_zip_extraction',
                        'vectorization_completed': True
                    }
            
            return response_result
            
        except Exception as e:
            logger.error(f"‚ùå Erro no processamento RAG: {e}")
            return {
                'success': False,
                'error': f'Erro interno: {str(e)}'
            }
    
    def _vectorize_licitacao(self, licitacao_id: str) -> Dict[str, Any]:
        """Vetoriza todos os documentos de uma licita√ß√£o"""
        try:
            # 1. Verificar e processar documentos automaticamente se necess√°rio
            documentos_result = self._ensure_documents_processed(licitacao_id)
            if not documentos_result['success']:
                return documentos_result
            
            # 2. Obter documentos do banco
            documentos = self.unified_processor.obter_documentos_licitacao(licitacao_id)
            
            if not documentos:
                return {
                    'success': False,
                    'error': 'Nenhum documento encontrado para vetoriza√ß√£o'
                }
            
            # 3. Verificar se j√° temos chunks vetorizados
            total_existing_chunks = 0
            for documento in documentos:
                chunks_count = self.vector_store.count_document_chunks(documento['id'])
                total_existing_chunks += chunks_count
                if chunks_count > 0:
                    logger.info(f"‚è≠Ô∏è Documento j√° vetorizado: {documento['titulo']} ({chunks_count} chunks)")
            
            # Se j√° temos chunks, considerar como sucesso
            if total_existing_chunks > 0:
                logger.info(f"‚úÖ Vetoriza√ß√£o j√° conclu√≠da: {len(documentos)} documentos, {total_existing_chunks} chunks existentes")
                return {
                    'success': True,
                    'message': 'Documentos j√° estavam vetorizados',
                    'processed_documents': len(documentos),
                    'total_chunks': total_existing_chunks,
                    'already_vectorized': True
                }
            
            # 4. Processar cada documento que ainda n√£o foi vetorizado
            total_chunks = 0
            processed_docs = 0
            
            logger.info(f"üìã Processando {len(documentos)} documentos para vetoriza√ß√£o...")
            
            for idx, documento in enumerate(documentos, 1):
                logger.info(f"üìÑ Processando documento {idx}/{len(documentos)}: {documento['titulo']}")
                # Verificar se j√° tem chunks vetorizados
                existing_chunks = self.vector_store.count_document_chunks(documento['id'])
                if existing_chunks > 0:
                    logger.info(f"‚è≠Ô∏è Documento j√° vetorizado: {documento['titulo']} ({existing_chunks} chunks)")
                    total_chunks += existing_chunks
                    processed_docs += 1
                    continue
                
                # Marcar como processando
                self._update_document_status(documento['id'], 'processando')
                
                try:
                    # Extrair texto
                    texto_completo = self.document_processor.extract_text_from_url(
                        documento['arquivo_nuvem_url']
                    )
                    
                    if not texto_completo:
                        self._update_document_status(documento['id'], 'erro')
                        continue
                    
                    # Salvar texto extra√≠do
                    self._save_extracted_text(documento['id'], texto_completo)
                    
                    # Criar chunks inteligentes
                    chunks = self.document_processor.create_intelligent_chunks(
                        texto_completo, documento['id']
                    )
                    
                    if not chunks:
                        self._update_document_status(documento['id'], 'erro')
                        continue
                    
                    # Gerar embeddings com fallback
                    chunk_texts = [chunk.text for chunk in chunks]
                    embeddings = self.embedding_service.generate_embeddings(chunk_texts)
                    
                    # Verificar se embeddings foram gerados com sucesso
                    if not embeddings or len(embeddings) != len(chunks):
                        logger.warning(f"‚ö†Ô∏è VoyageAI falhou para {documento['titulo']}, tentando fallback OpenAI...")
                        
                        # Tentar fallback com OpenAI (se dispon√≠vel)
                        try:
                            from matching.vectorizers import OpenAITextVectorizer
                            openai_vectorizer = OpenAITextVectorizer()
                            embeddings = []
                            
                            # Processar em lotes menores para OpenAI
                            batch_size = 50
                            for i in range(0, len(chunk_texts), batch_size):
                                batch = chunk_texts[i:i + batch_size]
                                batch_embeddings = openai_vectorizer.vectorize_batch(batch)
                                if batch_embeddings:
                                    embeddings.extend(batch_embeddings)
                                else:
                                    logger.error(f"‚ùå OpenAI fallback tamb√©m falhou no batch {i//batch_size + 1}")
                                    break
                            
                            if len(embeddings) != len(chunks):
                                logger.error(f"‚ùå Fallback OpenAI falhou para {documento['titulo']}")
                                logger.error(f"üìä Esperado: {len(chunks)} embeddings, Recebido: {len(embeddings)}")
                                self._update_document_status(documento['id'], 'erro_embedding')
                                continue
                            else:
                                logger.info(f"‚úÖ Fallback OpenAI funcionou para {documento['titulo']}")
                                
                        except Exception as fallback_error:
                            logger.error(f"‚ùå Fallback OpenAI falhou: {fallback_error}")
                            self._update_document_status(documento['id'], 'erro_embedding')
                            continue
                    
                    # Converter chunks para dicion√°rios
                    chunk_dicts = []
                    for chunk in chunks:
                        chunk_dict = {
                            'text': chunk.text,
                            'chunk_type': chunk.chunk_type,
                            'page_number': chunk.page_number,
                            'section_title': chunk.section_title,
                            'token_count': chunk.token_count,
                            'char_count': chunk.char_count,
                            'metadata': chunk.metadata or {}
                        }
                        chunk_dicts.append(chunk_dict)
                    
                    # Salvar no vector store
                    success = self.vector_store.save_chunks_with_embeddings(
                        documento['id'], licitacao_id, chunk_dicts, embeddings
                    )
                    
                    if success:
                        total_chunks += len(chunks)
                        processed_docs += 1
                        logger.info(f"‚úÖ Documento vetorizado: {documento['titulo']} ({len(chunks)} chunks)")
                    else:
                        self._update_document_status(documento['id'], 'erro')
                
                except Exception as e:
                    logger.error(f"‚ùå Erro ao vetorizar {documento['titulo']}: {e}")
                    self._update_document_status(documento['id'], 'erro')
                    continue
            
            if processed_docs > 0:
                logger.info(f"üéâ Vetoriza√ß√£o conclu√≠da: {processed_docs} documentos, {total_chunks} chunks")
                return {
                    'success': True,
                    'message': 'Documentos vetorizados com sucesso',
                    'processed_documents': processed_docs,
                    'total_chunks': total_chunks
                }
            else:
                return {
                    'success': False,
                    'error': 'Nenhum documento foi vetorizado com sucesso'
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro na vetoriza√ß√£o: {e}")
            return {
                'success': False,
                'error': f'Erro na vetoriza√ß√£o: {str(e)}'
            }
    
    def _answer_query(self, query: str, licitacao_id: str) -> Dict[str, Any]:
        """Responde uma query usando RAG"""
        try:
            start_time = time.time()
            
            # 1. Gerar embedding da query
            query_embedding = self.embedding_service.generate_single_embedding(query)
            
            if not query_embedding:
                return {
                    'success': False,
                    'error': 'Erro ao gerar embedding da consulta'
                }
            
            # 2. Buscar chunks relevantes (h√≠brida)
            chunks = self.vector_store.hybrid_search(
                query, query_embedding, licitacao_id, limit=12  # Buscar mais para reranking
            )
            
            if not chunks:
                return {
                    'success': False,
                    'error': 'Nenhum conte√∫do relevante encontrado nos documentos'
                }
            
            # 3. Aplicar reranking
            top_chunks = self.retrieval_engine.rerank_chunks(query, chunks, top_k=8)
            
            # 4. Obter informa√ß√µes da licita√ß√£o
            licitacao_info = self.unified_processor.extrair_info_licitacao(licitacao_id)
            
            # 5. Gerar resposta
            response_result = self.retrieval_engine.generate_response(
                query, top_chunks, licitacao_info
            )
            
            if response_result.get('error'):
                return {
                    'success': False,
                    'error': response_result['answer']
                }
            
            processing_time = time.time() - start_time
            
            # 6. Montar resultado final
            result = {
                'success': True,
                'answer': response_result['answer'],
                'query': query,
                'licitacao_id': licitacao_id,
                'chunks_used': response_result['chunks_used'],
                'sources': response_result['sources'],
                'processing_time': round(processing_time, 2),
                'model_response_time': response_result.get('response_time'),
                'cost_usd': response_result.get('cost_usd'),
                'model': response_result.get('model'),
                'cached': False
            }
            
            logger.info(f"‚úÖ Query respondida em {processing_time:.2f}s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao responder query: {e}")
            return {
                'success': False,
                'error': f'Erro ao processar consulta: {str(e)}'
            }
    
    def _ensure_documents_processed(self, licitacao_id: str) -> Dict[str, Any]:
        """
        üöÄ NOVA FUN√á√ÉO: Garante que documentos est√£o processados usando UnifiedDocumentProcessor recursivo
        Integra√ß√£o completa do processamento recursivo de ZIPs no fluxo RAG
        """
        try:
            logger.info(f"üîç Verificando documentos para licita√ß√£o: {licitacao_id}")
            
            # 1. Verificar se documentos j√° existem
            documentos_existem = self.unified_processor.verificar_documentos_existem(licitacao_id)
            
            if documentos_existem:
                # Verificar se documentos s√£o v√°lidos (n√£o corrompidos/vazios)
                documentos = self.unified_processor.obter_documentos_licitacao(licitacao_id)
                
                # Validar se temos documentos √∫teis
                documentos_validos = []
                for doc in documentos:
                    # Verificar se √© arquivo √∫til (PDF, DOC, etc) e n√£o est√° corrompido
                    if (doc.get('tipo_arquivo') in ['application/pdf', 'application/msword'] and 
                        doc.get('tamanho_arquivo', 0) > 1000):  # Arquivos > 1KB
                        documentos_validos.append(doc)
                
                if documentos_validos:
                    logger.info(f"‚úÖ {len(documentos_validos)} documentos v√°lidos j√° processados")
                    return {
                        'success': True,
                        'message': 'Documentos j√° processados',
                        'documentos_count': len(documentos_validos),
                        'action': 'documents_already_exist'
                    }
                else:
                    logger.warning(f"‚ö†Ô∏è Documentos existem mas s√£o inv√°lidos, reprocessando...")
                    # Limpar documentos inv√°lidos
                    self.unified_processor._limpar_documentos_licitacao(licitacao_id)
            
            # 2. Processar documentos usando nossa l√≥gica recursiva aprimorada
            logger.info("üì• Iniciando processamento recursivo de documentos...")
            
            # Primeiro, validar se a licita√ß√£o existe
            licitacao_info = self.unified_processor.extrair_info_licitacao(licitacao_id)
            if not licitacao_info:
                return {
                    'success': False,
                    'error': f'Licita√ß√£o {licitacao_id} n√£o encontrada no banco de dados',
                    'action': 'licitacao_not_found'
                }
            
            # Tentar processar documentos (m√©todo ass√≠ncrono)
            import asyncio
            result = asyncio.run(self.unified_processor.processar_documentos_licitacao(licitacao_id))
            
            if result['success']:
                logger.info(f"üéâ Processamento recursivo conclu√≠do: {result.get('documentos_processados', 0)} documentos")
                return {
                    'success': True,
                    'message': 'Documentos processados com sucesso usando extra√ß√£o recursiva',
                    'documentos_processados': result.get('documentos_processados', 0),
                    'storage_provider': result.get('storage_provider', 'supabase'),
                    'pasta_nuvem': result.get('pasta_nuvem'),
                    'action': 'documents_processed_recursive'
                }
            else:
                # Se falhou, tentar diagnosticar o problema
                error_detail = result.get('error', 'Erro desconhecido')
                logger.error(f"‚ùå Falha no processamento: {error_detail}")
                
                # Verificar se √© problema da API PNCP
                if 'API PNCP' in error_detail or 'HTTP' in error_detail:
                    return {
                        'success': False,
                        'error': f'Erro na API PNCP: {error_detail}',
                        'suggestion': 'Verifique se a licita√ß√£o possui documentos dispon√≠veis na API',
                        'action': 'api_error'
                    }
                
                # Outros erros
                return {
                    'success': False,
                    'error': f'Erro no processamento de documentos: {error_detail}',
                    'licitacao_info': {
                        'objeto': licitacao_info.get('objeto_compra', 'N/A')[:100] + '...',
                        'orgao': licitacao_info.get('orgao_entidade', 'N/A'),
                        'uf': licitacao_info.get('uf', 'N/A')
                    },
                    'action': 'processing_error'
                }
                
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao garantir documentos processados: {e}")
            return {
                'success': False,
                'error': f'Erro cr√≠tico no processamento: {str(e)}',
                'action': 'critical_error'
            }
    
    def _update_document_status(self, documento_id: str, status: str):
        """Atualiza status de processamento do documento"""
        try:
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE documentos_licitacao 
                        SET status_processamento = %s, updated_at = NOW()
                        WHERE id = %s
                    """, (status, documento_id))
                    conn.commit()
        except Exception as e:
            logger.error(f"‚ùå Erro ao atualizar status: {e}")
    
    def _save_extracted_text(self, documento_id: str, texto: str):
        """Salva texto extra√≠do no banco"""
        try:
            with self.db_manager.get_connection() as conn:
                with conn.cursor() as cursor:
                    cursor.execute("""
                        UPDATE documentos_licitacao 
                        SET texto_extraido = %s, updated_at = NOW()
                        WHERE id = %s
                    """, (texto, documento_id))
                    conn.commit()
        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar texto: {e}")
    
    def get_licitacao_stats(self, licitacao_id: str) -> Dict[str, Any]:
        """Retorna estat√≠sticas de uma licita√ß√£o"""
        try:
            status = self.vector_store.check_vectorization_status(licitacao_id)
            cache_stats = self.cache_manager.get_cache_stats()
            
            return {
                'licitacao_id': licitacao_id,
                'vectorization_status': status,
                'cache_stats': cache_stats,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao obter stats: {e}")
            return {'error': str(e)}
